---
title: "Data Quality Concepts"
---

Now that you can create comprehensive data dictionaries, you have a clear map of what your data should look like. But how do you know if your actual data matches that map? This is where data validation comes inâ€”systematically checking whether your data meets the expectations you've documented.

## Why Data Validation Matters

### Building on Your Documentation
Your data dictionary describes the ideal structure and content of your data. Data validation checks whether reality matches those expectations. Think of it as quality control for your research.

::: {.callout-note}
## Consider This
Remember the Palmer Penguins data dictionary you created? You documented that bill length should be measured in millimeters with values typically between 30-60mm. But what if some values in your dataset are 3000mm or negative numbers? Your analysis would be compromised even though your documentation is perfect.
:::

### The Research Impact
Poor data quality can:
- **Invalidate analyses**: Outliers or errors lead to wrong conclusions
- **Waste time**: Hours spent troubleshooting mysterious results caused by data issues
- **Undermine credibility**: Reviewers and collaborators lose confidence in findings
- **Prevent replication**: Others can't reproduce results with unreliable data

Good validation practices catch these issues early, when they're easy to fix.

## Types of Data Quality Issues

### Structural Problems
These violate the basic framework you documented:
- **Wrong data types**: Text in numeric columns, numbers stored as text
- **Missing required values**: Empty cells where data should exist
- **Invalid categories**: Misspelled factor levels or unexpected values
- **Out-of-range values**: Numbers that fall outside reasonable bounds

### Content Problems
These are more subtle but equally important:
- **Logical inconsistencies**: A penguin recorded as weighing 50,000 grams
- **Pattern violations**: ID numbers that don't follow expected formats
- **Duplicate records**: The same observation recorded multiple times
- **Measurement errors**: Values that are technically valid but scientifically implausible

## Validation as Quality Assurance

### Proactive vs. Reactive
**Reactive approach**: Discover data problems during analysis when they cause errors or strange results
**Proactive approach**: Systematically check data quality before analysis begins

The proactive approach saves time and increases confidence in your results.

### Levels of Validation

**Basic validation**: Does the data match its documented structure?
- Correct data types
- Expected number of variables and observations
- No completely empty columns or rows

**Content validation**: Are the values reasonable and consistent?
- Values fall within expected ranges
- Categorical variables contain only valid categories
- Relationships between variables make sense

**Domain validation**: Does the data make sense scientifically?
- Measurements are biologically/physically plausible
- Patterns align with known relationships
- Outliers have logical explanations

::: {.callout-tip}
## Building on Documentation
Notice how each level builds on your data dictionary work:
- Basic validation checks the structure you documented
- Content validation uses the ranges and categories you specified
- Domain validation relies on the scientific context you provided

Your documentation becomes the foundation for systematic quality checking.
:::

## Practical Example: Palmer Penguins Validation

Let's think through what validation might look like for our familiar dataset:

### Structural Checks
Based on your data dictionary, you'd verify:
- `species` contains only "Adelie", "Chinstrap", "Gentoo"
- `bill_length_mm` is numeric, not text
- Required measurements aren't missing for complete observations
- `year` contains only 2007, 2008, 2009

### Content Checks
Using the ranges you documented:
- Bill lengths between reasonable values (e.g., 30-60mm)
- Body mass within realistic ranges for penguins
- No negative measurements
- Sex coded consistently

### Domain Checks
Applying biological knowledge:
- Are bill dimensions reasonable for each species?
- Do body mass values align with known penguin biology?
- Are measurement combinations plausible?

::: {.callout-tip}
## Validation Planning Exercise
Think about a dataset from your own research:
1. What structural problems might occur during data collection or entry?
2. What content issues would be most problematic for your analyses?
3. What domain-specific knowledge should guide your quality checks?

Write down 3-4 specific validation rules you'd want to implement.
:::

## Validation Workflow Integration

### When to Validate
**During data collection**: Catch errors as they happen
**After data entry**: Before any analysis begins  
**Before major analyses**: Ensure data quality for important results
**Before sharing**: Verify data quality for others

### Documentation Connection
Your validation rules should directly connect to your data dictionary:
- Each documented range becomes a validation rule
- Each categorical variable's valid values become a check
- Each data type specification becomes a structural test

This creates a complete quality assurance system where documentation and validation work together.

## Benefits of Systematic Validation

### For Your Research
- **Increased confidence**: Know your data is reliable
- **Time savings**: Catch errors early rather than during analysis
- **Better results**: Analyses based on clean, verified data
- **Easier troubleshooting**: When something looks odd, it's probably real

### For Open Science
- **Transparency**: Others can see your quality control process
- **Reproducibility**: Quality checks can be replicated and verified
- **Trust**: Colleagues have confidence in shared data
- **Standards**: Contribute to better practices in your field

::: {.callout-tip}
## Reflection Question
How might implementing systematic data validation change your relationship with your research data? What would it mean for your confidence in results to know that data quality has been thoroughly verified?
:::

**Next**: We'll explore specific techniques for generating summary statistics that reveal data quality issues and support your validation efforts.